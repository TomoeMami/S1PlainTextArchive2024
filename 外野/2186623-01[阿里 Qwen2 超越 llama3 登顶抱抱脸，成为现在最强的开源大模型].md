
*****

####  Synopses6087  
##### 1#       楼主       发表于 2024-6-7 18:06

我文化水平低，看不懂高科技新闻，也不知道是新闻学魅力时刻还是阿里真的做了个能上新闻的东西
[https://www.zhihu.com/question/6 ... 1782473043581329408](https://www.zhihu.com/question/658307301/answer/3523276880?utm_psn=1782473043581329408)

*****

####  万恶淫猥手  
##### 2#       发表于 2024-6-7 18:12

搜一下通义千问，那公关稿可不少的

*****

####  Gotu  
##### 3#       发表于 2024-6-7 19:01

这里能任选两个AI对比

[https://www.modelscope.cn/studio ... ummary?fullScreen=1](https://www.modelscope.cn/studios/opencompass/CompassArena/summary?fullScreen=1)

*****

####  蛋饼  
##### 4#       发表于 2024-6-7 19:07

虽然是商稿，但qwen确实做得还行，而且是国内硕果仅存的几个开源llm之一了

*****

####  Synopses6087  
##### 5#         楼主| 发表于 2024-6-7 19:11

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145872&amp;ptid=2186623" target="_blank">蛋饼 发表于 2024-6-7 19:07</a>

虽然是商稿，但qwen确实做得还行，而且是国内硕果仅存的几个开源llm之一了</blockquote>
这个 qwen 和应用市场里免费的通义千问是一个 app 吗

*****

####  泰坦失足  
##### 6#       发表于 2024-6-7 19:13

Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近才放出8x22b。盘古模型到现在连Benchmark evaluation都没，就在那里看自媒体硬吹

*****

####  lukesweet  
##### 7#       发表于 2024-6-7 19:14

确实很不错，一些应用层实测甚至略强于GLM，阿里又是投资又是自己做开源，还都做得不错，真的猛，和朋友开玩笑说格局大得有点不像阿里

*****

####  7uly  
##### 8#       发表于 2024-6-7 19:15

qwen一直是开源大模型里第一梯队的 确实比较厉害

*****

####  蛋饼  
##### 9#       发表于 2024-6-7 19:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145922&amp;ptid=2186623" target="_blank">lukesweet 发表于 2024-6-7 19:14</a>

确实很不错，一些应用层实测甚至略强于GLM，阿里又是投资又是自己做开源，还都做得不错，真的猛，和朋友开 ...</blockquote>
毕竟modelscope也是阿里的，这个山寨hf能做起来价值大多了吧

*****

####  2474089352  
##### 10#       发表于 2024-6-7 19:40

qwen2开源协议改了，现在它真是国产开源之光了

[论坛助手,iPhone](https://bbs.saraba1st.com/2b/forum.php?mod=viewthread&amp;tid=2029836)

*****

####  naiveyan  
##### 11#       发表于 2024-6-7 19:44

阿里做得最好的一点是愿意反哺开源环境，国际上给流行的开源库都交了代码，墙内搞了魔搭和各种docker/pypi镜像，保证一发布全球都能用上（虽然qwen2在llama.cpp上还是翻车了），论使用方便仅次于llama。

单论国产开源权重模型，glm4，yi1.5也一直在出，智源的aquila虽然不出了但是bge还是在出，真闭源的也就baichuan，不至于“硕果仅存”

模型本身倒是中规中矩，体感文科类的功能（写作/翻译）上甚至比起上个版本最大的110b有退步，还得等超大杯

以及这次发布最有意思的点明明是画了个开源全模态模型，支持音像理解的饼，希望能比meta那边先出

*****

####  诚司  
##### 12#       发表于 2024-6-7 19:51

qwen2虽然榜上数据很强，但实际用感觉还是并没有llama3强

不过商不商稿的就没必要了，30b+尺寸的开源大模型都是全人类财富，阿里花钱做慈善还黑他何必呢

qwen2必然是最强开源中文大模型，也大概率是最强日语越南语韩语大模型，在业务级任务上逻辑推理能力提升很多了<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">

*****

####  ufo0000  
##### 13#       发表于 2024-6-7 20:01

写八股文，通义几乎秒杀市面其他

*****

####  诚司  
##### 14#       发表于 2024-6-7 20:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145908&amp;ptid=2186623" target="_blank">泰坦失足 发表于 2024-6-7 19:13</a>

Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近 ...</blockquote>
盘古有n个版本，我问过华为的人，他们基本觉得最高也就gpt3.5水平，而且还是175b的这个水平……

不过华为作为卖显卡的，这都不重要……

*****

####  weiyang  
##### 15#       发表于 2024-6-7 20:11

之前参加过一个会，阿里专家就说会坚持开源，因为无论搞什么样的大模型，都需要算力，都需要服务器，只要能推广，阿里怎么都有得赚

*****

####  alixsander  
##### 16#       发表于 2024-6-7 21:12

当然是真的，Qwen 1.5就很热门

*****

####  alixsander  
##### 17#       发表于 2024-6-7 21:13

<blockquote>泰坦失足 发表于 2024-6-7 19:13
Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近 ...</blockquote>
盘古就算了，向上管理骗骗可以，放出来一坨

*****

####  Risa  
##### 18#       发表于 2024-6-7 21:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146447&amp;ptid=2186623" target="_blank">weiyang 发表于 2024-6-7 20:11</a>

之前参加过一个会，阿里专家就说会坚持开源，因为无论搞什么样的大模型，都需要算力，都需要服务器，只要能 ...</blockquote>
所以这次不给32B和14B了，

32B单卡24G用，14B16G卡用，用爽了就不买服务器了。

*****

####  Van夫膜开  
##### 19#       发表于 2024-6-7 21:18

阿里这个模型确实很强之前的codeqwen 1.5 用7b的规模性能比deepseekcoder 33b还强，在evalplus排行榜上超越了gpt3.5。

今天好几个群里面反映对于复杂指令，qwen2 72b的性能居然比gpt4还强。

与其他团队不同，qwen团队不仅发布了模型，还一带把awq，gptq还有gguf文件都放出来了，真的很良心了。

*****

####  塔奇克马  
##### 20#       发表于 2024-6-7 21:19

什么时候有72b q2 量化啊

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.0.0.82-alpha

*****

####  Van夫膜开  
##### 21#       发表于 2024-6-7 21:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147091&amp;ptid=2186623" target="_blank">塔奇克马 发表于 2024-6-7 21:19</a>

什么时候有72b q2 量化啊

—— 来自 鹅球 v3.0.0.82-alpha</blockquote>
q2量化的准确率有点不忍直视了

*****

####  塔奇克马  
##### 22#       发表于 2024-6-7 21:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147152&amp;ptid=2186623" target="_blank">Van夫膜开 发表于 2024-6-7 21:25</a>

q2量化的准确率有点不忍直视了</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/125.png" referrerpolicy="no-referrer">和7B同样显存比咋样？

*****

####  ycjiang1337  
##### 23#       发表于 2024-6-7 21:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146428&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-7 20:09</a>

盘古有n个版本，我问过华为的人，他们基本觉得最高也就gpt3.5水平，而且还是175b的这个水平……

不过华为 ...</blockquote>
你看NV自己做过什么牛逼的大模型么？华为现在卖卡卖的飞起，数钱数到手抽筋，包括我司在内的国内头部互联网大厂基本都买了万卡昇腾集群，他真有那闲工夫不如好好把多机多卡通信再调好一点

*****

####  Risa  
##### 24#       发表于 2024-6-7 21:39

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147152&amp;ptid=2186623" target="_blank">Van夫膜开 发表于 2024-6-7 21:25</a>

q2量化的准确率有点不忍直视了</blockquote>
不会吧，模型参数规模越大，量化损失越小，

之前32B Q2都没啥太大损失，14B Q3以内也不会太大损失，

72B Q2应该性能不会达到疑惑率大幅升高的拐点后。

*****

####  ycjiang1337  
##### 25#       发表于 2024-6-7 21:39

Qwen1.5曾经是全球最强开源大模型，一直到LLama3出来才被超越。前两天Qwen2还没发布，推上就有很多白皮敲碗等更新了。

*****

####  ycjiang1337  
##### 26#       发表于 2024-6-7 21:41

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146146&amp;ptid=2186623" target="_blank">naiveyan 发表于 2024-6-7 19:44</a>

阿里做得最好的一点是愿意反哺开源环境，国际上给流行的开源库都交了代码，墙内搞了魔搭和各种docker/pypi ...</blockquote>
其实论使用方便已经超过LLama了，毕竟不用申请。之前LLama3首发的时候有些在硅谷的中国人甚至是从Modelscope上下载的LLama3权重，因为不用等审批

*****

####  omnitoken  
##### 27#       发表于 2024-6-7 21:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145908&amp;ptid=2186623" target="_blank">泰坦失足 发表于 2024-6-7 19:13</a>

Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近 ...</blockquote>
emmm 盘古基本没有对C的应用的, 实际上气象预测和政府相关项目才是大头

*****

####  treexper  
##### 28#       发表于 2024-6-7 21:49

llm到底看哪个榜，不是这个么？
[https://chat.lmsys.org/?leaderboard](https://chat.lmsys.org/?leaderboard)

*****

####  ycjiang1337  
##### 29#       发表于 2024-6-7 22:07

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147366&amp;ptid=2186623" target="_blank">treexper 发表于 2024-6-7 21:49</a>

llm到底看哪个榜，不是这个么？

https://chat.lmsys.org/?leaderboard</blockquote>
这是PVP榜，也算是其中一个

*****

####  Nanachi  
##### 30#       发表于 2024-6-8 00:43

价格也挺贵的，40元/百万

*****

####  fmketchup  
##### 31#       发表于 2024-6-8 00:52

mmp剩下就看老黄狗给不给顶级游戏卡加显存了。

*****

####  Ameyoru  
##### 32#       发表于 2024-6-8 01:12

在推上搜，居然看到了这个<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202406/08/011232m5zwfzgzzgg1ipt6.jpeg" referrerpolicy="no-referrer">" src="https://static.saraba1st.com/image/common/none.gif" referrerpolicy="no-referrer">

<strong>IMG_0343.jpeg</strong> (807.12 KB, 下载次数: 0)

下载附件

2024-6-8 01:12 上传

*****

####  ycjiang1337  
##### 33#       发表于 2024-6-8 01:21

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65149994&amp;ptid=2186623" target="_blank">Ameyoru 发表于 2024-6-8 01:12</a>

在推上搜，居然看到了这个</blockquote>
笑死，目前同级别支持日语的开源大模型完全没有，LLama3的多语言基本上是废的。有种给日本人喂屎的快感

*****

####  afer  
##### 34#       发表于 2024-6-8 01:23

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65149994&amp;ptid=2186623" target="_blank">Ameyoru 发表于 2024-6-8 01:12</a>

在推上搜，居然看到了这个</blockquote>
好！支持，无比强大！

*****

####  mimighost  
##### 35#       发表于 2024-6-8 01:42

看那个pvp榜吧，mmlu什么的我已经不看了，没意义，mistral之前说什么和gpt4一个水平，一用就露馅

甚至gpt4-o我也觉得不如之前gpt4的老版本，昨天一个简单的python函数就是写不对

*****

####  教练  
##### 36#       发表于 2024-6-8 01:47

阿里的模型是拿老黄的卡，还是菊花的卡训练的？老黄的话，短中期会不会遇到瓶颈（无法购买最新的卡，等等）

*****

####  ycjiang1337  
##### 37#       发表于 2024-6-8 02:00

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150228&amp;ptid=2186623" target="_blank">mimighost 发表于 2024-6-8 01:42</a>

看那个pvp榜吧，mmlu什么的我已经不看了，没意义，mistral之前说什么和gpt4一个水平，一用就露馅

甚至gpt4 ...</blockquote>
PVP榜比LLama3-70B低一点，考虑到语言可以认为起码是平手。另外GPT-4o确实比GPT-4有退化

*****

####  ycjiang1337  
##### 38#       发表于 2024-6-8 02:00

 本帖最后由 ycjiang1337 于 2024-6-8 02:02 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150264&amp;ptid=2186623" target="_blank">教练 发表于 2024-6-8 01:47</a>

阿里的模型是拿老黄的卡，还是菊花的卡训练的？老黄的话，短中期会不会遇到瓶颈（无法购买最新的卡，等等） ...</blockquote>
国内几家大厂都采购了万卡910B集群，多机多卡互联彻底调顺了之后基本能等价于万卡A100集群。

然后世界上最强大的大模型是GPT-4，在它训练的时候全世界最先进的卡就是A100，不存在更先进的——这是假设之后没有910C或者920的情况下。

*****

####  s1234y  
##### 39#       发表于 2024-6-8 02:08

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150351&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 02:00</a>
 国内几家大厂都采购了万卡910B集群，多机多卡互联彻底调顺了之后基本能等价于万卡A100集群。  然后世界上 ...</blockquote>
910b连qwen1.5的推理都没跑顺，mindie这玩意用起来就是一坨翔，还有精度丢失问题，更别说训练了。互联网都是各种渠道买或者租n卡，目前其实不缺卡。

*****

####  ycjiang1337  
##### 40#       发表于 2024-6-8 02:16

 本帖最后由 ycjiang1337 于 2024-6-8 02:27 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150394&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 02:08</a>

910b连qwen1.5的推理都没跑顺，mindie这玩意用起来就是一坨翔，还有精度丢失问题，更别说训练了。互联网 ...</blockquote>
你正好说反了，互联网大厂有合规要求反而不好买N卡。LLM现在是训练比推理简单，因为基于Paged Attention的超大吞吐量推理需要精细化调优——我司从Q2开始算法中台离线集群所有新上的资源组全都是910B，N卡资源组一个都没有。目前我们是用910B训大模型，用4090跑推理（后续可能升级到L20）。

起码对于Transformer类负载，目前我自己负责的模型负载已经全面迁到910B了，直接用torch2.1加上torch-npu，不存在任何精度和收敛性问题，单卡训练也不存在效率和兼容问题，实际速度（BERT全参和大模型LoRA微调）已经打平甚至超过A100了。然后我们部门自己用的大模型也已经在910B机器上完成了第一阶段持续预训练，同样不存在任何精度问题——哪怕是去年算法中台做验证的时候大模型相关的收敛性测试也都是一次通过的。目前我们用的算法中台提供的训练框架，只存在两个问题，一个是互联效率，另一个是多机模型并行的支持需要中台那边升级框架。当前过渡期我们正在把零散的中等负载都迁到910B上，把A100和H800腾出来集中使用。

<img src="https://img.saraba1st.com/forum/202406/08/022745bqq1x4wbra440ykl.png" referrerpolicy="no-referrer">

<strong>截屏2024-06-08 02.25.07.png</strong> (66.52 KB, 下载次数: 0)

下载附件

2024-6-8 02:27 上传


*****

####  ycjiang1337  
##### 41#       发表于 2024-6-8 02:26

 本帖最后由 ycjiang1337 于 2024-6-8 02:28 编辑 

风怒编辑

*****

####  s1234y  
##### 42#       发表于 2024-6-8 02:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150440&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 02:16</a>
 你正好说反了，互联网大厂有合规要求反而不好买N卡。LLM现在是训练比推理简单，因为基于Paged Attention的 ...</blockquote>
我了解互联网公司里边就美团买了910b，目前没见到他们训出来啥东西，有靠谱模型的公司里边智谱和讯飞和910b适配的还行。qwen1.5如果不用华为内部mindie rc2根本拉不起来推理，就算拉起来我手里还一大堆异常prompt华为没解决。ascend device plugin也有大坑。我手里就有推理精度下降的石锤证据，华为都认你不认<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">

整体看下来ascend工具链问题很多，还得再发现2-3年。

*****

####  ycjiang1337  
##### 43#       发表于 2024-6-8 02:29

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150521&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 02:28</a>

我了解互联网公司里边就美团买了910b，目前没见到他们训出来啥东西，有靠谱模型的公司里边智谱和讯飞和91 ...</blockquote>
那说明你消息过时了，去年10月华为来找我们宣讲的时候提到的标杆客户就有一个美团一个腾讯，而且用的还是CTR任务。

然后我司不是美团


*****

####  mimighost  
##### 44#       发表于 2024-6-8 02:41

单卡和集群不是一个东西，集群跑飞的情况可太多了

甚至用a100跑的通的，a800就跑不通，这两个的计算核心应该是完全一样的，这两天知乎就有帖子聊这个东西

*****

####  ycjiang1337  
##### 45#       发表于 2024-6-8 02:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150596&amp;ptid=2186623" target="_blank">mimighost 发表于 2024-6-8 02:41</a>

单卡和集群不是一个东西，集群跑飞的情况可太多了

甚至用a100跑的通的，a800就跑不通，这两个的计算核心应 ...</blockquote>
然而我们同样也跑通了16卡训练，训出来的模型已经投入使用了——所以你想说什么？

多机多卡目前张量并行确实存在问题，但这也是在排期之内的。另外大部分业务需要的模型都可以通过一个一个的8-16卡分组解决，这一来一回就能放出来很多A100，极端情况下把所有A100集中起来使用都足够解决很多问题了。


*****

####  诚司  
##### 46#       发表于 2024-6-8 02:48

美团、讯飞、京东、百度至少是都用了昇腾910b的

不过qwen，当时华为的人问了我一个奇怪的问题，为什么要用qwen？我说除了qwen（那时候还没command R+）还有几个开源的参数多的中英文都支持好的大模型？大概确实没适配吧<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">

*****

####  ycjiang1337  
##### 47#       发表于 2024-6-8 02:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150625&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 02:48</a>

美团、讯飞、京东、百度至少是都用了昇腾910b的

不过qwen，当时华为的人问了我一个奇怪的问题，为什么要用q ...</blockquote>
千问确实非常常用，基本上现在业务上用的开源大模型，差不多都是千问一统天下了，尤其电商领域。之前我们打比赛，KDD Cup第一阶段的测试集（英文电商问题）用千问1.5-7B性能碾压所有同尺寸模型，包括LLama3-8B。

目前实际结果跟预期完全相反，现在我们是910B训练，然后用N卡部署……部署的卡现在用的是4090，之后估计L20到了之后要换过去。


*****

####  诚司  
##### 48#       发表于 2024-6-8 02:53

刷了一下Chatbot Arena，发现qwen2低于预期的原因是中英之外的语言部分没上榜，感觉是Chatbot Arena对战里选模型的问题，因为qwen1.5排名都在那里，甚至llama2都在那里……

中英文都比command R+强，最后因为多语言没上榜导致在command R+后面……

cmd R+的多语言问题其实还挺大的，用中文提问回答能力比英文弱多了。这很正常，不过qwen从一开始就是中英双母语水平的，虽然有英文prompt蹦出来random Chinese token的问题，不过能力还是中英差不多（虽然可能是中英都烂），cmd R+就可能是英文烂中文更烂……


*****

####  ycjiang1337  
##### 49#       发表于 2024-6-8 02:57

 本帖最后由 ycjiang1337 于 2024-6-8 02:59 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150640&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 02:53</a>

刷了一下Chatbot Arena，发现qwen2低于预期的原因是中英之外的语言部分没上榜，感觉是Chatbot Arena对战里 ...</blockquote>
我感觉qwen的跨语言理解应该不是问题，KDD Cup那个英文电商测试集用qwen1.5-7B直接Zero-shot提交就能大比分吊打大多数其它模型……连LLama3-8B都被吊打了。

当时我们参赛的时候优化的方向甚至干脆就是如何在有限空间和时间限制里塞进去更大的qwen1.5……


*****

####  ycjiang1337  
##### 50#       发表于 2024-6-8 03:03

 本帖最后由 ycjiang1337 于 2024-6-8 03:05 编辑 

另外说到推理，最近算法中台在向我们推销他们的框架，号称910B能实现相当于A800-vllm方案的1.5倍性能。从他们的描述来看那个框架应该是基于MindIE的，但是里面的算子是他们自己写的，没有用华为的。目前我们部门暂时没有这么大的推理需求，所以还没接触。


*****

####  诚司  
##### 51#       发表于 2024-6-8 03:09

 本帖最后由 诚司 于 2024-6-8 03:11 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146146&amp;ptid=2186623" target="_blank">naiveyan 发表于 2024-6-7 19:44</a>

阿里做得最好的一点是愿意反哺开源环境，国际上给流行的开源库都交了代码，墙内搞了魔搭和各种docker/pypi ...</blockquote>
qwen1时代就有qwen VL和qwen audio，估计也就是整合重新练一下，audio还好，VL按现在的开放程度，不会放太好的模型出来，qwen VL max和开源版的水平就是天差地别……

更期待的还是gpt-o那种低时延的支持audio模态实时输出的模型，现在我用的tts，要么时延大要么语气生硬，毕竟vits模型是要把text全输入才能输出的……gpt-o这种活感觉不难做但是还是适合训大模型的人做

明明低时延聊天应该是一看就商业价值满满的路线，不知道为什么没人做……


*****

####  s1234y  
##### 52#       发表于 2024-6-8 03:24

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150528&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 02:29</a>
 首先你消息完全不灵通，去年10月华为来宣讲的时候提到的标杆互联网客户就至少有美团腾讯百度这仨，百度用9 ...</blockquote>
我们用客户的910b跑了基于qwen-14b的nl2sql子任务全参sft，训完跑测试集，和A100同样数据全参sft训出来的模型对比，sql正确率差了1.3%。我们冒充客户去和华为的人沟通，华为开始给的解释是说可能是权重转换完就有差异，后来发现我们不是客户自己人干脆不理我们了。
推理这么一个基本的事情现在都跑的稀碎，出任何小问题就让客户从驱动开始升级工具链全家桶，升级的版本还都是小版本号很接近的rc版本，这些都给我一种把客户当小白鼠的印象。
说实话有没有page attention这些优化我觉得都无所谓，就正常把模型推理跑起来，批量跑测试prompt不要有上百个乱码的case，把国内开源的hf格式的模型做好兼容，ascend做好这些真的就可以了。


*****

####  s1234y  
##### 53#       发表于 2024-6-8 03:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150677&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 03:09</a>
 qwen1时代就有qwen VL和qwen audio，估计也就是整合重新练一下，audio还好，VL按现在的开放程度，不会放太 ...</blockquote>
难度有点大，现在产业界的实践都是希望做到多模态进，然后意图识别，最后根据意图从录好的语音库里找录好的音频放出去<img src="https://static.saraba1st.com/image/smiley/face2017/021.png" referrerpolicy="no-referrer">


*****

####  诚司  
##### 54#       发表于 2024-6-8 03:37

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150709&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 03:28</a>

难度有点大，现在产业界的实践都是希望做到多模态进，然后意图识别，最后根据意图从录好的语音库里找录好 ...</blockquote>
anygpt那种就行……

现在speech这边已经可以离散token化了，离散token到语音这部分可以外挂，预测语音token对大语言模型来说感觉并不是一个很难的事。gpt-sovits用hubert token，再连一个sovits，时延主要是gpt部分不是实时预测的，对大模型应用来说，vit部分就算没gpto那样控制语气的水平，换个固定模板也可以应用了


*****

####  s1234y  
##### 55#       发表于 2024-6-8 04:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150722&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 03:37</a>
 anygpt那种就行……  现在speech这边已经可以离散token化了，离散token到语音这部分可以外挂，预测语音to ...</blockquote>
我们遇到的语音场景，端到端延迟不能超过300ms，否则用户会有明显感知，在这个时延上客户还希望能够检测到用户打断当前语音，我感觉这个需求听起来就很难落地<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">


*****

####  诚司  
##### 56#       发表于 2024-6-8 04:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150754&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 04:06</a>

我们遇到的语音场景，端到端延迟不能超过300ms，否则用户会有明显感知，在这个时延上客户还希望能够检测 ...</blockquote>
groq的llama3 70B，首token时延能达到200ms，理论上做到speech的transformer上会更快一些吧，按理说是可能达到的，不过现在语音合成的推理优化的工作确实感觉不多<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">


*****

####  東云研究所  
##### 57#       发表于 2024-6-8 09:00

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147265&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-7 21:38</a>
你看NV自己做过什么牛逼的大模型么？华为现在卖卡卖的飞起，数钱数到手抽筋，包括我司在内的国内头部互联 ...</blockquote>
?最基本的DLSS啊


*****

####  naiveyan  
##### 58#       发表于 2024-6-8 10:13

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147289&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-7 21:41</a>

其实论使用方便已经超过LLama了，毕竟不用申请。之前LLama3首发的时候有些在硅谷的中国人甚至是从Modelsc ...</blockquote>
llama主要是在开源领域号召力强，出了什么问题一群人第一时间饱和式修复，vllm在llama3发布第二天就专门发了个版本，llama.cpp为了llama3专门改了pre-tokenizer那一套代码，以及一大堆runtime从llama3开始才支持多个eos token，更不用说exl2和awq作者本人下场出量化版。至于需要申请使用，llama3一出一大群人第一时间做绕过申请的备份，我就是根据llama2时期的经验直接在nousresearch那边下的。

其他开源模型哪有这种待遇，遇到问题都是直接挂个help wanted issue开摆，跑不起来就是跑不起来，qwen使用方便全靠阿里愿意出人出力下场给开源项目做支持。


*****

####  子虚乌有  
##### 59#       发表于 2024-6-8 10:22

有没有guide教普通人怎么用这种开源大模型？


*****

####  yeo  
##### 60#       发表于 2024-6-8 10:31

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65152543&amp;ptid=2186623" target="_blank">子虚乌有 发表于 2024-6-8 10:22</a>
有没有guide教普通人怎么用这种开源大模型？</blockquote>
貌似也有gguf格式的参数，直接llama.cpp就能跑吧...建议知乎搜索<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">


*****

####  naiveyan  
##### 61#       发表于 2024-6-8 11:46

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150677&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 03:09</a>

qwen1时代就有qwen VL和qwen audio，估计也就是整合重新练一下，audio还好，VL按现在的开放程度，不会放太 ...</blockquote>
就是因为qwen以前开源过水平还不错的多模态模型所以才信他新画的饼啊

至于水平，现在整个多模态大模型领域都很草台，不管是之前的llava只训了一天就成了开源sota，还是最近的基于8b9b llm的多模态模型跑分超过规模大上百倍的gpt4v和qwen-vl-max都挺不可思议的，希望阿里（和字节？llava-next的blog里好多字节的人）之后能提升一下开源多模态模型的准入门槛吧


*****

####  宵待草  
##### 62#       发表于 2024-6-8 12:13

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65152543&amp;ptid=2186623" target="_blank">子虚乌有 发表于 2024-6-8 10:22</a>

有没有guide教普通人怎么用这种开源大模型？</blockquote>
[https://www.bentoml.com/blog/benchmarking-llm-inference-backends](https://www.bentoml.com/blog/benchmarking-llm-inference-backends)

主流的工具就这几个，基本都有教程


*****

####  诚司  
##### 63#       发表于 2024-6-8 12:18

 本帖最后由 诚司 于 2024-6-8 12:20 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65153426&amp;ptid=2186623" target="_blank">naiveyan 发表于 2024-6-8 11:46</a>

就是因为qwen以前开源过水平还不错的多模态模型所以才信他新画的饼啊

至于水平，现在整个多模态大模型领 ...</blockquote>
Minicpm虽然宣传里带上了超过qwen vl max，但benchmark比较的是qwen vl chat，textvqa之类的分数是没有qwen vl max高的……而且说白了minicpm和qwen vl max参数都不多，但也都能超过gpt4v，主要还是都有高质量的ocr精细标注数据……尤其minicpm，很明显是专门做了数据格式，页眉页脚标题都标好了……

这种模型按理说，如果同样标注量，对非ocr任务，对真实世界的认识肯定是不如cogvlm那种拿clip做视觉编码器的模型的(我猜gptv也是这种)，但现在这个标注量后者ocr也不行，image caption也没强到哪里去，先当玩具玩吧只能<img src="https://static.saraba1st.com/image/smiley/face2017/095.png" referrerpolicy="no-referrer">

现在的问题还是ocr的vlm太生产力了，百度和阿里都不想开源而，llava这套太玩具了，结果整个领域也就这样


*****

####  诚司  
##### 64#       发表于 2024-6-8 12:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65152543&amp;ptid=2186623" target="_blank">子虚乌有 发表于 2024-6-8 10:22</a>

有没有guide教普通人怎么用这种开源大模型？</blockquote>
koboldcpp，请

一个exe文件，自带前后端。后端基于llama.cpp，性能好，开了之后自带koboldcpp和openai server两个接口，前端接什么都行。

前端支持滑动context窗口，唯一缺点是丑

不过新模型可能用旧版本跑不了，多更新


*****

####  泰坦失足  
##### 65#       发表于 2024-6-8 12:33

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65153755&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 12:18</a>

Minicpm虽然宣传里带上了超过qwen vl max，但benchmark比较的是qwen vl chat，textvqa之类的分数是没有qwe ...</blockquote>
而且LLM有成熟的VLLM Ollama LLama.cpp快速部署，图像大模型连个类似的解决方案都没。再加上SOTA都不开源，不然我都想手搓个类似明日方舟MAA的工具帮我打各种手游了。


*****

####  诚司  
##### 66#       发表于 2024-6-8 12:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65153902&amp;ptid=2186623" target="_blank">泰坦失足 发表于 2024-6-8 12:33</a>

而且LLM有成熟的VLLM Ollama LLama.cpp快速部署，图像大模型连个类似的解决方案都没。再加上SOTA都不开源 ...</blockquote>
这个有啊，vllm和llama.cpp都是支持vlm的推理的，只不过只支持llava不支持cogvlm……minicpm倒是手搓了一个llama.cpp的版本

可以直接用llama.cpp加载minicpm的gguf，不过不要用量化版的，直接fp16走起，这破量化版给我弄麻了，这性能就不该放出来，ocr能力相当差


*****

####  ycjiang1337  
##### 67#       发表于 2024-6-8 13:13

 本帖最后由 ycjiang1337 于 2024-6-8 13:17 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150703&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 03:24</a>
我们用客户的910b跑了基于qwen-14b的nl2sql子任务全参sft，训完跑测试集，和A100同样数据全参sft训出来的 ...</blockquote>
所以你们这是国企客户？那估计也正常，等更新吧，没准之后你们客户就能用上我们给写的算子了。我们这边算法中台哪怕是对N卡也一样自己写算子，一个羊也是赶两个羊也是放。华为现在的玩法全面舔互联网客户，把互联网客户伺候好了，然后嫖客户写的算子。

—— 来自 HUAWEI HBN-AL80, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  naiveyan  
##### 68#       发表于 2024-6-8 13:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65153755&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 12:18</a>

Minicpm虽然宣传里带上了超过qwen vl max，但benchmark比较的是qwen vl chat，textvqa之类的分数是没有qwe ...</blockquote>
要是只有minicpm能以小博大那还能当成特例，可能面壁有独家高质量标注数据，关键一周之后智谱的glm4v9b（←虽然实际上是14b）也把这俩大的连带minicpm一锅烩了，那结论只能是整个领域开源闭源都是草台，没有半点护城河。

我看的是opencompass那个leaderboard，虽然minicpm均分主要是靠遥遥领先的ocrbench拉高的，但扣掉ocrbench只输qwen-vl-max0.1分，依然比gpt4v1106高，这个结果也很惊悚了。

*****

####  xing7673  
##### 69#       发表于 2024-6-8 13:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146447&amp;ptid=2186623" target="_blank">weiyang 发表于 2024-6-7 20:11</a>

之前参加过一个会，阿里专家就说会坚持开源，因为无论搞什么样的大模型，都需要算力，都需要服务器，只要能 ...</blockquote>
原来如此，基本就是微软的思路了

*****

####  诚司  
##### 70#       发表于 2024-6-8 13:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65154317&amp;ptid=2186623" target="_blank">naiveyan 发表于 2024-6-8 13:14</a>

要是只有minicpm能以小博大那还能当成特例，可能面壁有独家高质量标注数据，关键一周之后智谱的glm4v9b（ ...</blockquote>
其实qwen vl就这么大<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">

我倾向于认为qwen vl max也这么大，只不过数据上有差别而已


*****

####  ycjiang1337  
##### 71#       发表于 2024-6-8 13:21

 本帖最后由 ycjiang1337 于 2024-6-8 13:23 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150703&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 03:24</a>
我们用客户的910b跑了基于qwen-14b的nl2sql子任务全参sft，训完跑测试集，和A100同样数据全参sft训出来的 ...</blockquote>
Mind IE推理绝对不是什么“小事情”，因为这玩意本来就是对标vllm的高性能框架，paged attention是它存在的基础。

我这边的常规离线推理任务直接用transformers跑出来的结果没有任何问题，同一份数据训同一个lora模型，910B的结果跟H800完全一致。

—— 来自 HUAWEI HBN-AL80, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  s1234y  
##### 72#       发表于 2024-6-8 18:11

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65154313&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 13:13</a>
 所以你们这是国企客户？那估计也正常，等更新吧，没准之后你们客户就能用上我们给写的算子了。我们这边算 ...</blockquote>
我是阿里云的，华为听说我们是阿里云的直接就不理我们了哈哈，非常抵触我们<img src="https://static.saraba1st.com/image/smiley/face2017/001.png" referrerpolicy="no-referrer">


*****

####  tillnight  
##### 73#       发表于 2024-6-8 18:42

<blockquote>s1234y 发表于 2024-6-8 02:28
我了解互联网公司里边就美团买了910b，目前没见到他们训出来啥东西，有靠谱模型的公司里边智谱和讯飞和91 ...</blockquote>
哪怕按照开源情报的公开信息，去年第一批拿910b的就起码有百度。


*****

####  rmzxe  
##### 74#       发表于 2024-6-8 19:24

这么专业的么。。

—— 来自 Xiaomi 22041216C, Android 13上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  诚司  
##### 75#       发表于 2024-6-8 19:37

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65157364&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 18:11</a>

我是阿里云的，华为听说我们是阿里云的直接就不理我们了哈哈，非常抵触我们 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">华为这太搞了吧

我这边听的消息，互联网公司都是对华为感觉还可以，讯飞、百度都跟直接说过昇腾可以替代A100，和华为的口径一样，而国企都觉得昇腾一点也不行

没想到阿里因为阿里云的问题这么搞<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">


*****

####  Herreimu  
##### 76#       发表于 2024-6-8 21:01

试着拿上网本跑了一下，这tm mx250是怎么跑的动7b模型的<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">
不得不说，qwen2在写痔疮诗这方面比1.5强得不是一点半点<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">
<img src="https://p.sda1.dev/18/0a146a615f1122ee31c32507084005f9/image.jpg" referrerpolicy="no-referrer">

